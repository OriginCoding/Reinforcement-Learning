{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "692dccdd-2622-4d44-bc43-55e8cbebf859",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11087f78-66c4-4b7f-b147-ad0c4d0de917",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "environment_name = 'CartPole-v0'\n",
    "env = gym.make(environment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb04a982-fb93-40e2-a48a-eb79949ea88a",
   "metadata": {},
   "source": [
    "<h1> Loading Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e13df7d-8f49-442b-a27e-452cd4a69d9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:13.0\n",
      "Episode:2 Score:13.0\n",
      "Episode:3 Score:15.0\n",
      "Episode:4 Score:60.0\n",
      "Episode:5 Score:24.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1,episodes + 1):\n",
    "  state = env.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "\n",
    "  while not done:\n",
    "    env.render()\n",
    "    action = env.action_space.sample()\n",
    "    n_state, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "  print('Episode:{} Score:{}'.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0425849-1fd4-455b-8d19-93768994e416",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Work\\RL_01\\rl_01\\lib\\site-packages\\gym\\envs\\classic_control\\cartpole.py:150: UserWarning: \u001b[33mWARN: You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-0.09025163, -0.6074697 ,  0.2598153 ,  1.5231072 ], dtype=float32),\n",
       " 0.0,\n",
       " True,\n",
       " {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5a19953-70a4-41fb-a610-5cd75465a648",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "848d56e0-d2fd-43b9-b274-75f54c866d58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03bdb7-4e1e-4364-a5b2-b2d004aadce1",
   "metadata": {},
   "source": [
    "<h1> Train RL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a267dd12-5453-4922-a69a-dc3dd3f99daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training','Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42dfbbf5-93bf-42da-9d0f-f86f21958ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122fc331-2299-462b-b843-1ced85025c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(environment_name)\n",
    "env = DummyVecEnv([lambda:env])\n",
    "model = PPO('MlpPolicy',env, verbose=1, tensorboard_log = log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4070a-5f56-41c3-8c1a-e30641fd3a93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#PPO??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd7f3215-a924-4303-9990-c90e05dcc95e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_8\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 617  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 3    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 416          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 9            |\n",
      "|    total_timesteps      | 4096         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074408064 |\n",
      "|    clip_fraction        | 0.0567       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.688       |\n",
      "|    explained_variance   | -0.00237     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.52         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00798     |\n",
      "|    value_loss           | 52.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 372         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010778634 |\n",
      "|    clip_fraction        | 0.0808      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.128       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0198     |\n",
      "|    value_loss           | 35.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 352         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 23          |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007101194 |\n",
      "|    clip_fraction        | 0.0587      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.182       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.6        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0143     |\n",
      "|    value_loss           | 53.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 342         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 29          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011172285 |\n",
      "|    clip_fraction        | 0.0902      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.612      |\n",
      "|    explained_variance   | 0.357       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.2        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    value_loss           | 57          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 337          |\n",
      "|    iterations           | 6            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069417367 |\n",
      "|    clip_fraction        | 0.0561       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.598       |\n",
      "|    explained_variance   | 0.373        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 23           |\n",
      "|    n_updates            | 50           |\n",
      "|    policy_gradient_loss | -0.0135      |\n",
      "|    value_loss           | 61.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 334         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009037897 |\n",
      "|    clip_fraction        | 0.108       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.6        |\n",
      "|    explained_variance   | 0.703       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.78        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0144     |\n",
      "|    value_loss           | 42.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 332          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042749057 |\n",
      "|    clip_fraction        | 0.025        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.585       |\n",
      "|    explained_variance   | 0.456        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 11.5         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00583     |\n",
      "|    value_loss           | 71.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 329         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010022518 |\n",
      "|    clip_fraction        | 0.0797      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.578      |\n",
      "|    explained_variance   | 0.689       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.8        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00954    |\n",
      "|    value_loss           | 48.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 328          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0057074795 |\n",
      "|    clip_fraction        | 0.0364       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.564       |\n",
      "|    explained_variance   | 0.778        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.68         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0062      |\n",
      "|    value_loss           | 22.4         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1b603d41090>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94204981-194e-4a51-8d28-f3c3fd056b86",
   "metadata": {},
   "source": [
    "<h1> Save and Reload Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d53659-34c1-4fc1-a2ae-904ee9e1afa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PPO_Path = os.path.join('Training','Saved Models', 'PPO_Model_Cartpole')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "add8fe50-76d9-4009-9acb-754e792bd9dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9532869b-2c42-4ccf-b4f9-1aa45290c823",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3abd05c0-57e9-4049-bd85-2b5544544544",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(PPO_Path,env = env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733c3953-60d0-4f03-a892-861d5e46f1f8",
   "metadata": {},
   "source": [
    "<h1>Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a411eafc-806a-4d50-aa21-560f5227b741",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\RL_01\\rl_01\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy(model,env,n_eval_episodes=10,render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed05ec5d-b5fc-4fd7-af04-c9482e199838",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09edcc8d-1aaa-41a5-b345-ed6dc0716b8c",
   "metadata": {},
   "source": [
    "<h1> Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8403d2a-ab64-4532-be10-3fceebac6229",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 Score:[200.]\n",
      "Episode:2 Score:[200.]\n",
      "Episode:3 Score:[200.]\n",
      "Episode:4 Score:[200.]\n",
      "Episode:5 Score:[200.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5\n",
    "for episode in range(1,episodes + 1):\n",
    "  obs = env.reset()\n",
    "  done = False\n",
    "  score = 0\n",
    "\n",
    "  while not done:\n",
    "    env.render()\n",
    "    action, _= model.predict(obs)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    score += reward\n",
    "  print('Episode:{} Score:{}'.format(episode,score))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e7e92c2-e303-4096-8fe8-848b43aced69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "action, _ = model.predict(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff41ec47-e1c0-4080-850b-d5150de2a060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.00975204,  0.16618204, -0.00616501, -0.24897966]],\n",
       "       dtype=float32),\n",
       " array([1.], dtype=float32),\n",
       " array([False]),\n",
       " [{}])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b257746c-c310-4f90-a68a-669566449851",
   "metadata": {},
   "source": [
    "<h3> View Logs in Tensor Board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e0bc5e7-85b3-48ea-a7f3-e0f5e7ee7a4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_log_path = os.path.join(log_path,'PPO_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9614ee91-504c-408c-a699-9b3d860464e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO_8'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9082ef63-3f7f-452b-b560-a25726c5988c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir=[training_log_path]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "68a8ceaff0d0a82bbcb1e7f84433fc418d12562cce91f1093e17ecbcaba9ac00"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
